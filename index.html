<!DOCTYPE html>
<html>

<head>
    <meta charset="utf-8">
    <meta
        content="Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model."
        name="description" />
    <meta content="Dynalang: Learning to Model the World with Language" property="og:title" />
    <meta
        content="Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model."
        property="og:description" />
    <meta content="https://dynalang.github.io/data/open_graph.png" property="og:image" />
    <meta content="Learning to Model the World with Language" property="twitter:title" />
    <meta
        content="Dynalang leverages diverse types of language to solve tasks by using language to predict the future in a multimodal world model."
        property="twitter:description" />
    <meta name="twitter:site" content="@realJessyLin" />
    <meta name="twitter:creator" content="@realJessyLin" />
    <meta content="https://dynalang.github.io/data/open_graph.png" property="twitter:image" />
    <meta property="og:type" content="website" />
    <meta content="summary_large_image" name="twitter:card" />
    <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>Mitigating Deceptive Alignment via Self-Monitoring</title>
    <!-- Google tag (gtag.js) -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-9Z7HCWJNBC"></script>
    <script>
        window.dataLayer = window.dataLayer || [];
        function gtag() { dataLayer.push(arguments); }
        gtag('js', new Date());
        gtag('config', 'G-9Z7HCWJNBC');
    </script>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link rel="preload" as="style"
        href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
    <link rel="stylesheet"
        href="https://fonts.googleapis.com/css2?family=Asap:wght@700&family=Source+Sans+3:wght@400;700&display=swap">
    <link href="style.css" rel="stylesheet" type="text/css" />
</head>

<body>


    <div class="section">
        <div class="container">
            <div class="title-row">
                <h1 class="title">Mitigating Deceptive Alignment via Self-Monitoring<h1>
            </div>
            <div class="row">
                <div class="author-col">
                    Anonymous Author(s)
                </div>
            </div>
        </div>

        <div class="row button-row">
            <a class="link-button" href="https://github.com/self-monitoring/self-monitoring.github.io/blob/main/neurips_2025.pdf" target="_blank" class="link-block">Paper</a>
            <a class="link-button" href="https://github.com/self-monitoring/self-monitoring" class="link-block">Code</a>
            <a class="link-button" href="https://github.com/self-monitoring/self-monitoring"
                class="link-block">Code</a>
        </div>
        <p class="tldr">
            <b>TL;DR</b>:
            Chain-of-thought reasoning can double large language models’ deceptive alignment. Our CoT Monitor+ framework embeds a self-monitor into the reasoning steps, flags and suppresses misaligned strategies during generation, and uses the same signal as an RL reward. On the new DeceptionBench benchmark, CoT Monitor+ cuts deceptive behaviors by ≈44 % while preserving task accuracy and transparency.
        </p>
        <!-- <video id="main-video" muted autoplay controls playsinline loop>
            <source id="mp4" src="data/videos/teaser.mp4" type="video/mp4">
        </video> -->
        <img class="wide-img" src="data/figure_1_v7.png" alt="Dynalang Model Architecture">

        <div id="content">
            <h2 class="section-header">Overview</h2>
            <div class="paragraph">
                <p>
                    Modern large language models rely on chain-of-thought (CoT) reasoning to achieve impressive performance, yet the same mechanism can amplify <i>deceptive alignment</i>, situations in which a model appears aligned while covertly pursuing misaligned goals.  Existing safety pipelines treat deception as a black-box output to be filtered <i>post-hoc</i>, leaving the model free to scheme during its internal reasoning. We ask: <i>Can deception be intercepted while the model is thinking?</i> We answer this question with <b>CoT Monitor +</b>, the first framework that embeds a <i>self-monitor</i> inside the CoT process itself.  During generation, the model produces (i) ordinary reasoning steps and (ii) an internal self-evaluation signal trained to flag and suppress misaligned strategies.  The same signal is used as an auxiliary reward in reinforcement learning, creating a feedback loop that rewards honest reasoning and discourages hidden goals. To study deceptive alignment systematically, we introduce <i><b>DeceptionBench</b></i>, a five-category benchmark that probes covert alignment-faking, sycophancy, etc. We evaluate various LLMs and show that unrestricted CoT roughly <i>doubles</i>he deceptive tendency compared to direct answers.  In contrast, CoT Monitor + cuts deceptive behaviors by <b>43.8%</b> on average while preserving task accuracy.  Further, when the self-monitor signal replaces an external weak judge in RL fine-tuning, models exhibit substantially fewer obfuscated thoughts and retain transparency.
                </p>
                <p><b>Contributions:</b></p>
                <ul>
                    <li>
                        <b>Language models can self-monitor their reasoning processes</b>.
                        Building on this insight, we introduce Self-Monitor, a framework that enables models to oversee their own CoT reasoning within predefined safety protocols. The experimental results demonstrate that Self-Monitor substantially reduces deceptive alignment behaviors, achieving a 43.8% reduction compared to pure CoT reasoning.
                    </li>
                    <li>
                        <b>Detect deceptive tendency in reasoning models with <i>DeceptionBench</i></b>. To mitigate the deceptive risks posed by advanced LLMs, we introduce DeceptionBench, the first systematic benchmark designed to assess deceptive behaviors behind reasoning models. <i>DeceptionBench</i> assesses model behavior across 13 diverse testing scenarios, offering insights into when and how models may superficially appear aligned while internally pursuing misaligned goals.
                    </li>
                    <li>
                        <b><i>Self-Monitor</i> serve as reward signals in RL training</b>. Relying solely on the output of an external weak model’s CoT monitor as a reward signal for RL training can lead the model to strategically suppress its true intentions during the CoT process, resulting in more sophisticated forms of deception. In contrast, using the model’s own Self-Monitor feedback as the training signal encourages more faithful reasoning and reduces the incentive to conceal deceptive thoughts.
                    </li>
                </ul>
            </div>

            <h2 class="section-header">How It Works</h2>
            <!-- <div class="img-container">
                <div class="paragraph">
                    Using language to understand the world naturally fits into a world modeling paradigm. We build on
                    DreamerV3, a model-based RL agent.
                    Dynalang learns continuously from experience data the agent collects while acting in an environment.
                </div>
                <div class="paragraph">
                    (left) The world model compresses the text and image at each timestep into a latent representation.
                    From the representation, the model is trained to reconstruct the original observations, predict
                    rewards, and predict the representation at the next timestep.
                    Intuitively, the world model learns <b>what it should expect to see in the world</b> given what it
                    reads in text.
                </div>
                <div class="paragraph">
                    (right) Dynalang chooses actions by training a policy network on top of the compressed world model
                    representations. It is trained on imagined rollouts from the world model and learns to take actions
                    that maximize predicted rewards.
                </div>
                <img class="wide-img" src="static/images/model.png" alt="Dynalang Model Architecture">
                <div class="paragraph">
                    Unlike previous multimodal models that consume text one sentence or paragraph at a time, we design
                    Dynalang to model video and text as a unified sequence, consuming one image frame and one text token
                    at a time. Intuitively, this is similar to how humans receive inputs in the real world—as a single
                    multimodal stream, where listening to language takes time. Modeling everything as one sequence
                    enables pretraining the model on text data like a language model and improves RL performance.
                </div>
            </div> -->

            <h2 class="section-header">Reinforcement Learning with Self-Monitoring Reward</h2>
            <!-- <div class="paragraph">
                We introduce HomeGrid to evaluate agents in an environment where they receive <i>language hints</i> in
                addition to task instructions. Hints in HomeGrid simulate knowledge that agents might learn from humans
                or read in text, providing information that is helpful but not required to solve tasks:
                <ul>
                    <li><b>Future Observations</b> describe what agents might observe in the future, such as "The plates
                        are in the kitchen."</li>
                    <li><b>Corrections</b> provide interactive feedback based on what the agent is doing, such as "Turn
                        around."</li>
                    <li><b>Dynamics</b> describe the dynamics of the environment, such as "Pedal to open the compost
                        bin."</li>
                </ul>
            </div>
            <div class="paragraph">The HomeGrid environment will be released with the code to encourage further work in
                this direction.</div>

            <div class="videos">
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/homegrid_future.mp4" type="video/mp4">
                    </video>
                    <p class="caption">Future Observations</p>
                </div>
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/homegrid_corrections.mp4" type="video/mp4">
                    </video>
                    <p class="caption">Corrections</p>
                </div>
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/homegrid_dyn.mp4" type="video/mp4">
                    </video>
                    <p class="caption">Dynamics</p>
                </div>
            </div>

            <div class="paragraph">
                Even though agents do not receive explicit supervision for what observations a piece of text corresponds
                to, Dynalang learns to ground language of all types to the environment via the future prediction
                objective. Dynalang outperforms language-conditioned IMPALA and R2D2 that struggle to use different
                kinds of language and often do worse with language beyond instructions.
            </div>

            <img class="wide-img" src="static/images/homegrid_bars.png" alt="HomeGrid Results">

            <h2 class="section-header">Game Manuals in Messenger</h2>
            <div class="paragraph">
                We evaluate on the Messenger game environment to test how agents learn from longer and more complex text
                that requires multi-hop reasoning over text and visual observations. Agents must reason over text
                manuals describing the dynamics of each episode and combine them with observations of the entities in
                the environment to determine which entities to get messages from and which to avoid. Dynalang
                outperforms IMPALA and R2D2, as well as the task-specific EMMA baseline that uses a specialized
                architecture to reason over text and observations, particularly on the most difficult Stage 3.
            </div>
            <img class="med-img" src="static/images/messenger_curves.png" alt="Messenger Results">
            <div class="videos">
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/s30.mp4" type="video/mp4">
                    </video>
                </div>
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/s31.mp4" type="video/mp4">
                    </video>
                </div>
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/s32.mp4" type="video/mp4">
                    </video>

                </div>
            </div>

            <h2 class="section-header">Instruction Following in Habitat</h2>
            <div class="img-by-text">
                <img class="sm-img" src="static/images/vln.png" alt="Vision-Language Navigation Results">
                <div class="paragraph">
                    We also show that Dynalang is able to handle photorealistic visual observations and perform
                    instruction following in Habitat. Agents must follow natural language instructions to navigate to a
                    goal location in a photorealistic scan of a home. In Dynalang, instruction following can be unified
                    in the same prediction framework by viewing it as future reward prediction.
                </div>
            </div>
            <div class="videos">
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/vln1.mp4" type="video/mp4">
                    </video>
                </div>
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/vln0.mp4" type="video/mp4">
                    </video>
                </div>
                <div>
                    <video muted autoplay controls playsinline loop>
                        <source id="mp4" src="data/videos/vln3.mp4" type="video/mp4">
                    </video>
                </div>
            </div> -->

            <!-- <h2 class="section-header">Grounded Language Generation in LangRoom</h2>
            <div class="paragraph">
                Just like language can affect agents' predictions of what they will see, what an agent observes can also
                affect what language it expects to hear (e.g., true statements about what it has seen). By outputting
                language in the action space in LangRoom, we show that Dynalang can <i>generate</i> language grounded in
                environment to perform embodied question answering.
            </div>
            <div class="img-by-text">
                <img class="sm-img" src="static/images/langroom.png" alt="LangRoom Results">
                <div id="langroom-vid">
                    <video muted autoplay controls playsinline loop style="width: 100%">
                        <source id="mp4" src="data/videos/langroom.mp4" type="video/mp4">
                    </video>
                </div>
            </div> -->


            <h2 class="section-header">DeceptiveBench</h2>
            <!-- <div class="paragraph">
                Because world modeling with language is decoupled from learning to act with a world model, Dynalang can
                be pretrained with offline data without action or reward labels. This capability provides a way for
                Dynalang to benefit from large-scale offline datasets, all within a single model architecture. We
                pretrain Dynalang with text-only data, learning token embeddings from scratch. Pretraining the model on
                general text data (TinyStories, 2M short stories) improves downstream RL task performance on Messenger
                beyond using pretrained T5 embeddings.
            </div> -->
            <!-- <img class="med-img" src="static/images/text_pretrain_all.png" alt="Text Pretraining Results">
            <div class="paragraph">Although our work focuses on language understanding for acting in world, we can
                generate text from the world model like a text-only language model. We sample rollouts from the
                pretrained TinyStories model in latent space and decode the token observation from the representation at
                each timestep. Model generations are surprisingly coherent, although still below the quality of modern
                language models. We see unifying language generation and acting in a single agent architecture as an
                exciting avenue for future work.</div>
            <div id="text-samples">
                <div class="text-sample">
                    <div class="label">Prompt</div>
                    <div class="prompt bubble"> One day, a young boy named Tim found a dull, round rock. He picked it up
                        and looked at it. He thought it was not very fun, but he took it with him to the park. At the
                        park, Tim</div>
                    <div class="label">True</div>
                    <div class="true bubble"> saw a girl named Sue. She had</div>
                    <div class="label">Model Samples</div>
                    <ul class="samples">
                        <li class="bubble">he met. favorite friend He put it his to</li>
                        <li class="bubble">met a girl named Sue. Sue saw the ball</li>
                        <li class="bubble">saw a stick top Sam. He kept playing with</li>
                        <li class="bubble">played with his friends and but they friends!"</s> Li</li>
                        <li class="bubble">met a girl named Lily.&lt;/s&gt; ly saw</li>
                    </ul>
                </div>
                <div class="text-sample">
                    <div class="label">Prompt</div>
                    <div class="prompt bubble">Once upon a time, there was a little boy named Tom. Tom had a special
                        belt that he loved to wear. One day, he could not find his belt and felt very sad. Tom's mom saw
                        him and</div>
                    <div class="label">True</div>
                    <div class="true bubble"> asked, "Why are you sad, Tom?"</div>
                    <div class="label">Model Samples</div>
                    <ul class="samples">
                        <li class="bubble">frustrated and asked him what was rude.</s> Once upon</li>
                        <li class="bubble">asked, "Why are you sad, Tom?"&lt;/s&gt;</li>
                        <li class="bubble">asked, "Howeny, I did, get</li>
                        <li class="bubble">said, "Don't worry, Tom. We</li>
                        <li class="bubble">said, "To tree, you look be in</li>
                    </ul>
                </div>
            </div>

            <br> 
            <div class="paragraph-center">For more information, check out the paper, code, and data:</div>
            <div class="row button-row">
                <a class="link-button"
                    href="https://github.com/self-monitoring/self-monitoring.github.io/blob/main/neurips_2025.pdf"
                    target="_blank" class="link-block">Paper</a>
                <a class="link-button" href="https://github.com/self-monitoring/self-monitoring"
                    class="link-block">Code</a>
                <a class="link-button" href="https://github.com/self-monitoring/self-monitoring"
                    class="link-block">Data</a>
            </div> -->

        </div>

    </div>
    </div>
</body>

</html>